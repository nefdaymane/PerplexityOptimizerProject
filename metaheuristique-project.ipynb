{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10163400,"sourceType":"datasetVersion","datasetId":6276106},{"sourceId":104587,"sourceType":"modelInstanceVersion","modelInstanceId":72260,"modelId":76277}],"dockerImageVersionId":30827,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"nirugidla/sample-data\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T19:16:21.281773Z","iopub.execute_input":"2024-12-20T19:16:21.282134Z","iopub.status.idle":"2024-12-20T19:16:25.972697Z","shell.execute_reply.started":"2024-12-20T19:16:21.282105Z","shell.execute_reply":"2024-12-20T19:16:25.971594Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Path to dataset files: /kaggle/input/sample-data\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-27b\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T19:20:11.377963Z","iopub.execute_input":"2024-12-20T19:20:11.378355Z","iopub.status.idle":"2024-12-20T19:20:13.917329Z","shell.execute_reply.started":"2024-12-20T19:20:11.378327Z","shell.execute_reply":"2024-12-20T19:20:13.915407Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/gemma-2/transformers/gemma-2-27b/2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nimport logging\nfrom tqdm import tqdm\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n@dataclass\nclass OptimizationConfig:\n    # Configuration GA-PSO\n    population_size: int = 20  # Réduit pour plus de rapidité\n    mutation_rate: float = 0.2  # Augmenté pour plus d'exploration\n    max_generations: int = 30   # Réduit pour plus de rapidité\n    \n    # Configuration SA\n    initial_temperature: float = 50.0\n    cooling_rate: float = 0.85\n    max_sa_iterations: int = 50  # Limite le nombre d'itérations\n    \n    # Paramètres généraux\n    timeout_seconds: int = 300  # 5 minutes max par texte\n    perplexity_threshold: float = 10.0  # Seuil d'acceptation\n    batch_size: int = 8  # Pour le calcul de perplexité\n\nclass OptimizedTextProcessor:\n    def __init__(self, model_path: str, config: OptimizationConfig):\n        self.config = config\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map=\"auto\",\n            torch_dtype=torch.bfloat16\n        )\n        self.model.eval()\n        \n    @torch.no_grad()\n    def batch_calculate_perplexity(self, sequences: List[str]) -> List[float]:\n        try:\n            # Tokenisation en batch\n            inputs = self.tokenizer(\n                sequences,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=512\n            )\n            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n            \n            # Calcul des perplexités en batch\n            outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n            losses = outputs.loss.view(-1).cpu().numpy()\n            return np.exp(losses).tolist()\n            \n        except Exception as e:\n            logging.error(f\"Batch perplexity calculation error: {str(e)}\")\n            return [float('inf')] * len(sequences)\n\n    def optimize_text(self, text: str) -> str:\n        words = text.split()\n        if len(words) <= 1:\n            return text\n\n        start_time = time.time()\n        best_sequence = words\n        best_perplexity = float('inf')\n        \n        def time_exceeded():\n            return time.time() - start_time > self.config.timeout_seconds\n\n        # Fonction rapide de mutation\n        def quick_mutate(sequence):\n            if len(sequence) <= 2:\n                return sequence\n            idx1, idx2 = random.sample(range(len(sequence)), 2)\n            sequence[idx1], sequence[idx2] = sequence[idx2], sequence[idx1]\n            return sequence\n\n        # Optimisation principale avec early stopping\n        population = [words[:] for _ in range(self.config.population_size)]\n        \n        for generation in range(self.config.max_generations):\n            if time_exceeded():\n                break\n                \n            # Mutations en batch\n            mutated = [quick_mutate(seq[:]) for seq in population]\n            sequences = [\" \".join(seq) for seq in mutated]\n            \n            # Calcul des perplexités en batch\n            perplexities = []\n            for i in range(0, len(sequences), self.config.batch_size):\n                batch = sequences[i:i + self.config.batch_size]\n                perplexities.extend(self.batch_calculate_perplexity(batch))\n            \n            # Mise à jour de la meilleure solution\n            for seq, perp in zip(mutated, perplexities):\n                if perp < best_perplexity:\n                    best_sequence = seq[:]\n                    best_perplexity = perp\n                    if best_perplexity < self.config.perplexity_threshold:\n                        return \" \".join(best_sequence)\n            \n            # Sélection des meilleurs pour la prochaine génération\n            population = [seq for _, seq in sorted(zip(perplexities, mutated))[:self.config.population_size]]\n            \n            # Injection de diversité\n            if generation % 5 == 0:\n                population[-2:] = [words[:] for _ in range(2)]\n\n        return \" \".join(best_sequence)\n\ndef optimize_dataset(data_path: str, model_path: str, output_path: str):\n    config = OptimizationConfig()\n    processor = OptimizedTextProcessor(model_path, config)\n    \n    # Chargement des données\n    data = pd.read_csv(data_path)\n    results = []\n    \n    # Traitement avec barre de progression\n    with tqdm(total=len(data), desc=\"Optimizing texts\") as pbar:\n        for idx, row in data.iterrows():\n            try:\n                optimized_text = processor.optimize_text(row['text'])\n                results.append({\n                    \"id\": row['id'],\n                    \"text\": optimized_text\n                })\n            except Exception as e:\n                logging.error(f\"Error processing row {idx}: {str(e)}\")\n                results.append({\n                    \"id\": row['id'],\n                    \"text\": row['text']\n                })\n            pbar.update(1)\n            \n            # Sauvegarde intermédiaire tous les 10 textes\n            if len(results) % 10 == 0:\n                pd.DataFrame(results).to_csv(output_path, index=False)\n    \n    # Sauvegarde finale\n    pd.DataFrame(results).to_csv(output_path, index=False)\n    logging.info(f\"Optimization completed. Results saved to {output_path}\")\n\nif __name__ == \"__main__\":\n    optimize_dataset(\n        data_path=\"/kaggle/input/sample-data/sample_submission.csv\",\n        model_path=\"/kaggle/input/gemma-2/transformers/gemma-2-27b/2/\",\n        output_path='/kaggle/working/optimized_submission.csv'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:36:47.740582Z","iopub.execute_input":"2024-12-21T17:36:47.740990Z","iopub.status.idle":"2024-12-21T18:05:43.233171Z","shell.execute_reply.started":"2024-12-21T17:36:47.740963Z","shell.execute_reply":"2024-12-21T18:05:43.231739Z"}},"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 24/24 [00:27<00:00,  1.16s/it]\nOptimizing texts: 100%|██████████| 6/6 [28:21<00:00, 283.53s/it]\n","output_type":"stream"}],"execution_count":2}]}