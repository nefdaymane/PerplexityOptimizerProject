{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10163400,"sourceType":"datasetVersion","datasetId":6276106},{"sourceId":104587,"sourceType":"modelInstanceVersion","modelInstanceId":72260,"modelId":76277}],"dockerImageVersionId":30827,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"nirugidla/sample-data\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T19:16:21.281773Z","iopub.execute_input":"2024-12-20T19:16:21.282134Z","iopub.status.idle":"2024-12-20T19:16:25.972697Z","shell.execute_reply.started":"2024-12-20T19:16:21.282105Z","shell.execute_reply":"2024-12-20T19:16:25.971594Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Path to dataset files: /kaggle/input/sample-data\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-27b\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T19:20:11.377963Z","iopub.execute_input":"2024-12-20T19:20:11.378355Z","iopub.status.idle":"2024-12-20T19:20:13.917329Z","shell.execute_reply.started":"2024-12-20T19:20:11.378327Z","shell.execute_reply":"2024-12-20T19:20:13.915407Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/gemma-2/transformers/gemma-2-27b/2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nimport logging\nfrom tqdm import tqdm\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n@dataclass\nclass OptimizationConfig:\n    # Configuration GA-PSO\n    population_size: int = 20  \n    mutation_rate: float = 0.2  \n    max_generations: int = 30   \n    \n    # Configuration SA\n    initial_temperature: float = 50.0\n    cooling_rate: float = 0.85\n    max_sa_iterations: int = 50  \n    \n    # Paramètres généraux\n    timeout_seconds: int = 300  \n    perplexity_threshold: float = 10.0  \n    batch_size: int = 8  \n\nclass OptimizedTextProcessor:\n    def __init__(self, model_path: str, config: OptimizationConfig):\n        self.config = config\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map=\"auto\",\n            torch_dtype=torch.bfloat16\n        )\n        self.model.eval()\n        \n    @torch.no_grad()\n        \n\n    def batch_calculate_perplexity(self, sequences: List[str]) -> List[float]:\n        try:\n            inputs = self.tokenizer(\n                sequences,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=512\n            )\n            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n            outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n            # Assurez-vous que losses est un tenseur 1D\n            losses = outputs.loss.view(-1).cpu().numpy()\n            return np.exp(losses).tolist()\n        \n        except Exception as e:\n            logging.error(f\"Batch perplexity calculation error: {str(e)}\")\n            return [float('inf')] * len(sequences)\n        \n        \n    def optimize_text(self, text: str) -> Tuple[str, float]:\n        words = text.split()\n        if len(words) <= 1:\n            # Calculer la perplexité du texte original si trop court\n            perplexity = self.batch_calculate_perplexity([text])[0]\n            return text, perplexity\n\n        start_time = time.time()\n        best_sequence = words\n        best_perplexity = float('inf')\n    \n        def time_exceeded():\n            return time.time() - start_time > self.config.timeout_seconds\n\n        def quick_mutate(sequence):\n            if len(sequence) <= 2:\n                return sequence\n            idx1, idx2 = random.sample(range(len(sequence)), 2)\n            new_sequence = sequence.copy()\n            new_sequence[idx1], new_sequence[idx2] = new_sequence[idx2], new_sequence[idx1]\n            return new_sequence\n\n        # Calcul initial de la perplexité\n        original_text = \" \".join(words)\n        initial_perplexity = self.batch_calculate_perplexity([original_text])[0]\n        best_perplexity = initial_perplexity\n    \n        population = [words[:] for _ in range(self.config.population_size)]\n    \n        try:\n            for generation in range(self.config.max_generations):\n                if time_exceeded():\n                    break\n                \n                # Mutations\n                mutated = [quick_mutate(seq[:]) for seq in population]\n                sequences = [\" \".join(seq) for seq in mutated]\n            \n                # Calcul des perplexités en batch\n                perplexities = []\n                for i in range(0, len(sequences), self.config.batch_size):\n                    batch = sequences[i:i + self.config.batch_size]\n                    batch_perplexities = self.batch_calculate_perplexity(batch)\n                    perplexities.extend(batch_perplexities)\n            \n                # Mise à jour de la meilleure solution\n                for idx, (seq, perp) in enumerate(zip(mutated, perplexities)):\n                    if perp < best_perplexity and not np.isinf(perp):\n                        best_sequence = seq[:]\n                        best_perplexity = perp\n                        logging.info(f\"New best perplexity: {best_perplexity}\")\n            \n                # Sélection\n                population = [seq for _, seq in sorted(zip(perplexities, mutated))[:self.config.population_size]]\n            \n                if generation % 5 == 0:\n                    population[-2:] = [words[:] for _ in range(2)]\n\n        except Exception as e:\n            logging.error(f\"Error during optimization: {str(e)}\")\n            return original_text, initial_perplexity\n\n        # Toujours retourner un tuple (str, float)\n        optimized_text = \" \".join(best_sequence)\n        return optimized_text, best_perplexity\n\n\ndef optimize_dataset(data_path: str, model_path: str, output_path: str):\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n    \n    config = OptimizationConfig()\n    processor = OptimizedTextProcessor(model_path, config)\n    \n    data = pd.read_csv(data_path)\n    results = []\n    \n    with tqdm(total=len(data), desc=\"Optimizing texts\") as pbar:\n        for idx, row in data.iterrows():\n            try:\n                text = row['text']\n                optimized_text, perplexity_score = processor.optimize_text(text)\n                \n                result = {\n                    \"id\": row['id'],\n                    \"text\": optimized_text,\n                    \"original_text\": text,\n                    \"perplexity_score\": float(perplexity_score)  # Conversion explicite en float\n                }\n                results.append(result)\n                \n                logging.info(f\"Text {idx} processed - Score: {perplexity_score}\")\n                \n            except Exception as e:\n                logging.error(f\"Error processing row {idx}: {str(e)}\")\n                results.append({\n                    \"id\": row['id'],\n                    \"text\": row['text'],\n                    \"original_text\": row['text'],\n                    \"perplexity_score\": float('inf')\n                })\n            \n            pbar.update(1)\n            \n            # Sauvegarde intermédiaire\n            if (idx + 1) % 10 == 0:\n                pd.DataFrame(results).to_csv(output_path, index=False)\n    \n    # Sauvegarde finale\n    final_df = pd.DataFrame(results)\n    final_df.to_csv(output_path, index=False)\n    \n    # Statistiques finales\n    valid_scores = final_df['perplexity_score'][~np.isinf(final_df['perplexity_score'])]\n    if len(valid_scores) > 0:\n        logging.info(f\"\"\"\n        Optimization completed:\n        - Mean perplexity: {valid_scores.mean():.2f}\n        - Median perplexity: {valid_scores.median():.2f}\n        - Min perplexity: {valid_scores.min():.2f}\n        - Max perplexity: {valid_scores.max():.2f}\n        \"\"\")\n    else:\n        logging.error(\"No valid perplexity scores calculated!\")\n        \nif __name__ == \"__main__\":\n    optimize_dataset(\n        data_path=\"/kaggle/input/sample-data/sample_submission.csv\",\n        model_path=\"/kaggle/input/gemma-2/transformers/gemma-2-27b/2/\",\n        output_path='/kaggle/working/optimized_submission.csv'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T20:52:13.430780Z","iopub.execute_input":"2024-12-21T20:52:13.431241Z","iopub.status.idle":"2024-12-21T21:21:32.318822Z","shell.execute_reply.started":"2024-12-21T20:52:13.431199Z","shell.execute_reply":"2024-12-21T21:21:32.317363Z"}},"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 24/24 [00:21<00:00,  1.12it/s]\nOptimizing texts: 100%|██████████| 6/6 [28:50<00:00, 288.46s/it]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T21:30:40.493610Z","iopub.execute_input":"2024-12-21T21:30:40.493964Z","iopub.status.idle":"2024-12-21T21:30:40.500824Z","shell.execute_reply.started":"2024-12-21T21:30:40.493939Z","shell.execute_reply":"2024-12-21T21:30:40.499570Z"}},"outputs":[],"execution_count":11}]}